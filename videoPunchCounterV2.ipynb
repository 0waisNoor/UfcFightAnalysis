{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ufc punch counter version 2</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>This code maintains a seperate punch count for each fighter using the punch classification model trained in train.ipynb and another pose detection model called MoveNet for fighter classification</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "\n",
    "import cv2\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import DatasetCatalog,MetadataCatalog\n",
    "import os\n",
    "\n",
    "import imageio \n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Calculations and Deep Learning library\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import video and classification\n",
    "\n",
    "vc = cv2.VideoCapture(\"mcgregorvsdiaz.mp4\")\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "vw = cv2.VideoWriter(\"output4.avi\",fourcc,20,(1280,720))\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES=3\n",
    "cfg.OUTPUT_DIR = 'faster_rcnn_R_50_FPN_3x\\\\lr=0.001,itr=3000'\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "predictor = DefaultPredictor(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COLORS\n",
    "cyan = (255, 255, 0)\n",
    "magenta = (255, 0, 255)\n",
    "EDGE_COLORS = {\n",
    "    (0, 1): magenta,\n",
    "    (0, 2): cyan,\n",
    "    (1, 3): magenta,\n",
    "    (2, 4): cyan,\n",
    "    (0, 5): magenta,\n",
    "    (0, 6): cyan,\n",
    "    (5, 7): magenta,\n",
    "    (7, 9): cyan,\n",
    "    (6, 8): magenta,\n",
    "    (8, 10): cyan,\n",
    "    (5, 6): magenta,\n",
    "    (5, 11): cyan,\n",
    "    (6, 12): magenta,\n",
    "    (11, 12): cyan,\n",
    "    (11, 13): magenta,\n",
    "    (13, 15): cyan,\n",
    "    (12, 14): magenta,\n",
    "    (14, 16): cyan\n",
    "}\n",
    "\n",
    "# MODEL\n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = model.signatures[\"serving_default\"]\n",
    "\n",
    "# PARAMETERS\n",
    "WIDTH = 256\n",
    "HEIGHT = 128\n",
    "\n",
    "\n",
    "# FUNCTION TO GET SHORTS COLOR\n",
    "def getShortsColor(frame):\n",
    "    threshold=0.11\n",
    "    \"\"\"Draws the keypoints on a image frame\"\"\"\n",
    "    initial_shape = frame.shape\n",
    "    image = cv2.resize(frame, (HEIGHT,WIDTH))\n",
    "    # Resize to the target shape and cast to an int32 vector\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(image, WIDTH, HEIGHT), dtype=tf.int32)\n",
    "    # Create a batch (input tensor)\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    # Perform inference\n",
    "    results = movenet(input_image)\n",
    "    keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    keypoints = keypoints[0]\n",
    "    # Denormalize the coordinates : multiply the normalized coordinates by the input_size(width,height)\n",
    "    denormalized_coordinates = np.squeeze(np.multiply(keypoints, [WIDTH,HEIGHT,1]))\n",
    "    # Get the keypoint of the left thigh\n",
    "    lowerBodyPoints = [denormalized_coordinates[11]]\n",
    "    for keypoint in lowerBodyPoints:\n",
    "        # Unpack the keypoint values : y, x, confidence score\n",
    "        keypoint_y, keypoint_x, keypoint_confidence = keypoint\n",
    "\n",
    "\n",
    "        if keypoint_confidence > threshold:\n",
    "            \"\"\"\"\n",
    "            Use keypoints to calculate top right and bottom left coordinates of region of interest\n",
    "            Region of interest = portion of fighter's shorts to indetify color for distinction\n",
    "            \"\"\"\n",
    "            topRight = (keypoint_x-3,keypoint_y-3)\n",
    "            bottomLeft = (keypoint_x+3,keypoint_y+12)\n",
    "\n",
    "    resultImage = frame[int(topRight[1]):int(bottomLeft[1]),int(topRight[0]):int(bottomLeft[0])]\n",
    "    return resultImage\n",
    "\n",
    "# FUNCTION TO COMPARE SHORT COLORS\n",
    "    \n",
    "def compareColors(base,shorts1):\n",
    "    base = np.array(base)\n",
    "    shorts1 = np.array(shorts1)\n",
    "\n",
    "    diff1 = np.abs(base - shorts1)\n",
    "    diff1 = np.mean(diff1.ravel())\n",
    "    return diff1\n",
    "\n",
    "    \n",
    "# GET BASE SHORT COLORS - these will be compared with shorts colors of fighters in the video\n",
    "# for classification purposes.\n",
    "base1=[]\n",
    "base2=[]\n",
    "\n",
    "def getShortsColor(frame):\n",
    "    threshold=0.1\n",
    "    \"\"\"Draws the keypoints on a image frame\"\"\"\n",
    "    initial_shape = frame.shape\n",
    "    image = cv2.resize(frame, (HEIGHT,WIDTH))\n",
    "    # Resize to the target shape and cast to an int32 vector\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(image, WIDTH, HEIGHT), dtype=tf.int32)\n",
    "    # Create a batch (input tensor)\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    # Perform inference\n",
    "    results = movenet(input_image)\n",
    "    keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    keypoints = keypoints[0]\n",
    "    # Denormalize the coordinates : multiply the normalized coordinates by the input_size(width,height)\n",
    "    denormalized_coordinates = np.squeeze(np.multiply(keypoints, [WIDTH,HEIGHT,1]))\n",
    "    #Iterate through the points\n",
    "    lowerBodyPoints = [denormalized_coordinates[11]]\n",
    "    for keypoint in lowerBodyPoints:\n",
    "        # Unpack the keypoint values : y, x, confidence score\n",
    "        keypoint_y, keypoint_x, keypoint_confidence = keypoint\n",
    "        \n",
    "        topRight = (keypoint_x-3,keypoint_y-3)\n",
    "        bottomLeft = (keypoint_x+3,keypoint_y+20)\n",
    "    \n",
    "    resultImage = image[int(topRight[1]):int(bottomLeft[1]),int(topRight[0]):int(bottomLeft[0])]\n",
    "\n",
    "    if resultImage.shape!=(23, 6, 3): #the shape may not be equal if the point detected in near the edges of the image\n",
    "        return np.ones((23, 6, 3)),[int(topRight[0]),int(topRight[1]),int(bottomLeft[0]),int(bottomLeft[1])]\n",
    "\n",
    "    return resultImage,[int(topRight[0]),int(topRight[1]),int(bottomLeft[0]),int(bottomLeft[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters for the classification\n",
    "\n",
    "counter = 0\n",
    "punches1=0\n",
    "punches2=0\n",
    "buffer=[]\n",
    "fighterDetectionBuffer=[]\n",
    "\n",
    "fighter1 = cv2.imread(\"fighters\\\\fighter1.png\")\n",
    "fighter2 = cv2.imread(\"fighters\\\\fighter2.png\")\n",
    "\n",
    "# these two shorts colors will be compared with frames in the video\n",
    "base1,frame1 = getShortsColor(fighter1)\n",
    "base2,frame2 = getShortsColor(fighter2)\n",
    "\n",
    "plt.imshow(base2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "while True:\n",
    "    ret,img = vc.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    counter+=1\n",
    "    # print(counter)\n",
    "\n",
    "    # make the prediction\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    #get  bounding box, labels and confidence levels\n",
    "    boxes = outputs[\"instances\"].pred_boxes.tensor.tolist()\n",
    "    labels=outputs[\"instances\"].pred_classes.tolist()\n",
    "    conf_lvls=outputs[\"instances\"].scores.tolist()\n",
    "\n",
    "    #value format=(boxes,conf_lvl)\n",
    "    punchBoxes=[]\n",
    "    for i,box in enumerate(boxes):\n",
    "        if labels[i]==2:\n",
    "            punchBoxes.append([box,conf_lvls[i]])\n",
    "\n",
    "\n",
    "    # add to punch count if punch is detected in 3 to 10 consecutive frames\n",
    "    # this is because a punch may last several seconds depending on the fps of the video\n",
    "    # I found it was 3-10 for this video, you can adjust it if not accurate\n",
    "    if len(punchBoxes)>=1:         \n",
    "        # if buffer is empty or has previous frame\n",
    "        if len(buffer)==0 or buffer[len(buffer)-1]+1==counter:\n",
    "            buffer.append(counter)\n",
    "        # clear buffer if frame is not consecutive\n",
    "        elif buffer[len(buffer)-1]+1!=counter:\n",
    "            if len(buffer)>=3 and len(buffer)<=10:\n",
    "                if mode(fighterDetectionBuffer)==1:\n",
    "                    punches1+=1\n",
    "                else:\n",
    "                    punches2+=1\n",
    "\n",
    "            buffer=[]\n",
    "            fighterDetectionBuffer=[]\n",
    "            buffer.append(counter)\n",
    "\n",
    "        # classify punching fighter as figter 1 or fighter2 and increment the correspoding punch count:\n",
    "        if len(buffer)==10:\n",
    "            if len(buffer)>=3 and len(buffer)<=10:\n",
    "                if mode(fighterDetectionBuffer)==1:\n",
    "                    punches1+=1\n",
    "                else:\n",
    "                    punches2+=1\n",
    "\n",
    "            buffer=[]\n",
    "            fighterDetectionBuffer=[]\n",
    "            buffer.append(counter)\n",
    "\n",
    "        #maintain fightDetectionBuffer\n",
    "        for box in punchBoxes:\n",
    "            box = box[0] #remove confidence level from punchBoxes\n",
    "            xmin,ymin,xmax,ymax = box\n",
    "            xmin = int(xmin)\n",
    "            ymin = int(ymin)\n",
    "            xmax = int(xmax)\n",
    "            ymax = int(ymax)\n",
    "\n",
    "            fighter = img[ymin:ymax,xmin:xmax,::]\n",
    "            color,shortsCoordinates = getShortsColor(img)\n",
    "\n",
    "            # classify punching fighter as 1 or 2 by comparing their shorts colors with the base short colors\n",
    "            # and choosing the one with the lesser difference\n",
    "            diff1 = compareColors(base1,color)\n",
    "            diff2 = compareColors(base2,color)\n",
    "\n",
    "        if diff1<diff2:\n",
    "            fighterDetectionBuffer.append(1)\n",
    "        else:\n",
    "            fighterDetectionBuffer.append(2)\n",
    "\n",
    "        text = \"Color difference of last puncher: {}(1) and {}(2) \".format(round(diff1),round(diff2))\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_size=1\n",
    "        font_color = (0,255,0)\n",
    "        font_thickness=2\n",
    "        position = (50,150)\n",
    "\n",
    "    #print stats for debug purposes\n",
    "    print(counter,buffer,fighterDetectionBuffer)      \n",
    "\n",
    "    \n",
    "    # draw these boxes on the image\n",
    "    for box in punchBoxes:\n",
    "        cv2.rectangle(img,(int(box[0][0]),int(box[0][1])),(int(box[0][2]),int(box[0][3])),(255,0,0),2)\n",
    "\n",
    "    cv2.putText(img,\"Fighter 1: {}\".format(punches1),(50,50),cv2.FONT_HERSHEY_SIMPLEX,2,(255,0,0),2)\n",
    "    cv2.putText(img,\"Fighter 2: {}\".format(punches2),(50,100),cv2.FONT_HERSHEY_SIMPLEX,2,(255,0,0),2)\n",
    "\n",
    "    # save the frames with detected punches for ebug purposes\n",
    "    '''\n",
    "    if len(punchBoxes)>0:\n",
    "        cv2.imwrite(\"results\\\\{}.jpg\".format(counter),img)\n",
    "    '''\n",
    "\n",
    "    vw.write(img)\n",
    "\n",
    "vc.release()\n",
    "vw.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
